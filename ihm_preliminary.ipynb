{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary experiment - distilled dataset for in-hospital mortality prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Dependencies.\n",
    "\n",
    "For cuda, define `device` that'll be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Custom libs. **Always re-run the following code block after modifying `utils`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.report' from '/project/ruishanl_1185/EHR-Distillation/utils/report.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import preprocess, dataset, network, train, report\n",
    "importlib.reload(preprocess)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(network)\n",
    "importlib.reload(train)\n",
    "importlib.reload(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Compute statistics that'll be used for imputation and dataloading.\n",
    "\n",
    "Statistics can be load from saved pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FROM_SAVED = True\n",
    "STAT_PKL_DIR = \"./saved_data/stats/\"\n",
    "if not os.path.exists(STAT_PKL_DIR):\n",
    "    os.makedirs(STAT_PKL_DIR)\n",
    "\n",
    "categorical_numcls = {  # how many classes are there for categorical classes\n",
    "    \"capillary_refill_rate\": 2,\n",
    "    \"glascow_coma_scale_eye_opening\": 4,\n",
    "    \"glascow_coma_scale_motor_response\": 6,\n",
    "    \"glascow_coma_scale_total\": 13,\n",
    "    \"glascow_coma_scale_verbal_response\": 5,\n",
    "}\n",
    "\n",
    "pkl_path = os.path.join(STAT_PKL_DIR, \"ihm_preliminary.pkl\")\n",
    "if os.path.exists(pkl_path) and LOAD_FROM_SAVED:\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        continuous_avgs_train, continuous_stds_train, categorical_modes_train = pickle.load(f)\n",
    "else:\n",
    "    continuous_avgs_train, continuous_stds_train, categorical_modes_train =  preprocess.compute_feature_statistics(\n",
    "        ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/train/\",\n",
    "        feature_dict=preprocess.mimic3_benchmark_variable_dict\n",
    "        )\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump((continuous_avgs_train, continuous_stds_train, categorical_modes_train), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean all data by resampling, imputating and masking.\n",
    "\n",
    "**Running this block once is enough.**\n",
    "(Cleaned data will be saved at ./data/mimic3/ihm_preliminary/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.preprocess_ihm_timeseries_files(\n",
    "    ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/train/\",\n",
    "    output_dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    feature_dict=preprocess.mimic3_benchmark_variable_dict,\n",
    "    normal_value_dict=continuous_avgs_train|categorical_modes_train\n",
    "    )\n",
    "preprocess.preprocess_ihm_timeseries_files(\n",
    "    ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/test/\",\n",
    "    output_dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    feature_dict=preprocess.mimic3_benchmark_variable_dict,\n",
    "    normal_value_dict=continuous_avgs_train|categorical_modes_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Define original mimic3 benchmark training and evaluation datasets for IHM objective.\n",
    "\n",
    "Apply mask / balance to the dataset here.\n",
    "\n",
    "You may have to re-run this block after modifying dataloader-related codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in the dataset: \n",
      "(tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0668,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0489,  ...,  0.0220,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000,  0.0980,  ..., -0.0870,  4.1913,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.1293,  ..., -0.0870,  4.1913,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0176,  ..., -0.0870,  4.1913,  0.0000]]), tensor(0))\n",
      "Feature tensor shape: torch.Size([48, 42])\n",
      "First item in the dataset: \n",
      "(tensor([[ 1.0000,  0.0000,  0.0310,  ..., -0.1730, -0.8523,  0.0391],\n",
      "        [ 1.0000,  0.0000,  0.0578,  ..., -0.1615, -0.8523,  0.0391],\n",
      "        [ 1.0000,  0.0000,  0.0623,  ..., -0.1615, -0.8523,  0.0391],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000,  0.0132,  ...,  0.0335, -0.5163,  0.0521],\n",
      "        [ 1.0000,  0.0000,  0.0087,  ...,  0.0335, -0.5163,  0.0521],\n",
      "        [ 1.0000,  0.0000,  0.0400,  ...,  0.0335, -0.5163,  0.0521]]), tensor(0))\n",
      "Feature tensor shape: torch.Size([48, 42])\n",
      "Input tensor shape: torch.Size([48, 42])\n"
     ]
    }
   ],
   "source": [
    "# Pay attention to balance and mask settings\n",
    "\n",
    "BALANCE = False\n",
    "MASK = False\n",
    "\n",
    "train_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=BALANCE,\n",
    "    mask=MASK,\n",
    "    )\n",
    "print(f\"First item in the dataset: \\n{train_set[0]}\")\n",
    "print(f\"Feature tensor shape: {train_set[0][0].shape}\")\n",
    "\n",
    "test_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    dstype=\"test\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=BALANCE,\n",
    "    mask=MASK,\n",
    "    )\n",
    "print(f\"First item in the dataset: \\n{test_set[0]}\")\n",
    "print(f\"Feature tensor shape: {test_set[0][0].shape}\")\n",
    "\n",
    "input_shape = train_set[0][0].shape\n",
    "print(f\"Input tensor shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the label distribution in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datapoints: 17904\n",
      "Label 0 ratio: 0.8646112600536193\n",
      "Label 1 ratio: 0.1353887399463807\n"
     ]
    }
   ],
   "source": [
    "label_0_cnt = 0\n",
    "label_1_cnt = 1\n",
    "for _, label in test_set:\n",
    "    if label > 0.5:\n",
    "        label_1_cnt += 1\n",
    "    else:\n",
    "        label_0_cnt += 1\n",
    "print(f\"Total datapoints: {label_0_cnt + label_1_cnt}\")\n",
    "print(f\"Label 0 ratio: {label_0_cnt / (label_0_cnt + label_1_cnt)}\")\n",
    "print(f\"Label 1 ratio: {label_1_cnt / (label_0_cnt + label_1_cnt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’­ Evaluating model capacity on training objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an 1D CNN and save the best performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper params\n",
    "ihm_epoch = 100\n",
    "ihm_batch_size = 256\n",
    "ihm_lr = 1e-3\n",
    "ihm_wd = 1e-3\n",
    "\n",
    "# train 1D CNN\n",
    "\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(SYN_DATA_SAVE_DIR):\n",
    "    os.makedirs(SYN_DATA_SAVE_DIR)\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_loader = DataLoader(train_set, ihm_batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "model = network.IHMPreliminary1DCNN(input_shape=input_shape).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(ihm_epoch), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", train_loader, model, criterion, optimizer, device=DEVICE)\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, criterion, device=DEVICE)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_1dcnn_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(SYN_DATA_SAVE_DIR, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(SYN_DATA_SAVE_DIR) if f.startswith(f'ihm_1dcnn_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(SYN_DATA_SAVE_DIR, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_description(f\"Training on original task, epoch {e}\\ntrain loss = {train_loss}, train acc = {train_acc}\\ntest loss = {test_loss}, test acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an MLP and save the best performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper params\n",
    "ihm_epoch = 100\n",
    "ihm_batch_size = 256\n",
    "ihm_lr = 1e-3\n",
    "ihm_wd = 1e-3\n",
    "\n",
    "# train MLP\n",
    "\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(SYN_DATA_SAVE_DIR):\n",
    "    os.makedirs(SYN_DATA_SAVE_DIR)\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_loader = DataLoader(train_set, ihm_batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "model = network.IHMPreliminaryMLP(input_shape=input_shape).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(ihm_epoch), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", train_loader, model, criterion, optimizer, device=DEVICE)\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, criterion, device=DEVICE)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_mlp_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(SYN_DATA_SAVE_DIR, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(SYN_DATA_SAVE_DIR) if f.startswith(f'ihm_mlp_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(SYN_DATA_SAVE_DIR, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_description(f\"Training on original task, epoch {e}\\ntrain loss = {train_loss}, train acc = {train_acc}\\ntest loss = {test_loss}, test acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines: train the model with randomly sampled subset of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample set size = 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021213a9c6bd42faae8ecc9365596512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training on sampled dataset (size=20):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     47\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mepoch(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, sample_loader, model, loss_fn, optimizer, device\u001b[39m=\u001b[39mDEVICE) \u001b[39m# attention: using sampled dataset\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mepoch(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, test_loader, model, loss_fn, device\u001b[39m=\u001b[39;49mDEVICE)\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m train_loss \u001b[39m<\u001b[39m min_loss:\n\u001b[1;32m     50\u001b[0m         filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mihm_\u001b[39m\u001b[39m{\u001b[39;00mMODEL\u001b[39m}\u001b[39;00m\u001b[39m_spc\u001b[39m\u001b[39m{\u001b[39;00mN_SAMPLES_PER_CLS\u001b[39m}\u001b[39;00m\u001b[39m_e\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m_trl\u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_tel\u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m/project/ruishanl_1185/EHR-Distillation/utils/train.py:16\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(mode, dataloader, net, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     net\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, datum \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     17\u001b[0m     feat \u001b[39m=\u001b[39m datum[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     lab \u001b[39m=\u001b[39m datum[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# choose model\n",
    "MODEL = [\"1dcnn\", \"mlp\"][1]\n",
    "\n",
    "# define hyper params\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WD = 1e-3\n",
    "\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(SYN_DATA_SAVE_DIR):\n",
    "    os.makedirs(SYN_DATA_SAVE_DIR)\n",
    "\n",
    "# define num sampled datapoints\n",
    "N_SAMPLES_PER_CLS = 10\n",
    "\n",
    "# Sample from class 0\n",
    "ts_class_0, lab_class_0 = train_set.random_sample_from_class(n_samples=N_SAMPLES_PER_CLS, cls=0, no_duplicate=True)\n",
    "# Sample from class 1\n",
    "ts_class_1, lab_class_1 = train_set.random_sample_from_class(n_samples=N_SAMPLES_PER_CLS, cls=1, no_duplicate=True)\n",
    "# Concatenate the time series data along the first dimension (batch size)\n",
    "ts_real = torch.cat((ts_class_0, ts_class_1), dim=0).to(DEVICE)\n",
    "# Concatenate the labels along the 0th dimension\n",
    "lab_real = torch.cat((lab_class_0, lab_class_1), dim=0).to(DEVICE)\n",
    "# print(ts_real.shape, lab_real.shape) # batch_size * num_time_steps * num_features\n",
    "\n",
    "sample_set = dataset.TensorDataset(ts_real, lab_real)\n",
    "print(f\"Sample set size = {len(sample_set)}\")\n",
    "\n",
    "# define dataloaders\n",
    "NUM_WORKERS = 8\n",
    "sample_loader = DataLoader(sample_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "if MODEL == \"1dcnn\":\n",
    "    model = network.IHMPreliminary1DCNN(input_shape=input_shape).to(DEVICE)\n",
    "else:\n",
    "    model = network.IHMPreliminaryMLP(input_shape=input_shape).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(EPOCH), desc=f\"Training on sampled dataset (size={2*N_SAMPLES_PER_CLS})\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", sample_loader, model, loss_fn, optimizer, device=DEVICE) # attention: using sampled dataset\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, loss_fn, device=DEVICE)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_{MODEL}_spc{N_SAMPLES_PER_CLS}_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(SYN_DATA_SAVE_DIR, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(SYN_DATA_SAVE_DIR) if f.startswith(f'ihm_{MODEL}_spc{N_SAMPLES_PER_CLS}_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(SYN_DATA_SAVE_DIR, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_postfix({\"train loss\": f\"{train_loss:.4f}\",\n",
    "                      \"train acc\": f\"{train_acc*100:.2f}%\",\n",
    "                      \"test loss\": f\"{test_loss:.4f}\",\n",
    "                      \"test acc\": f\"{test_acc*100:.2f}%\",\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a saved model and evaluate on evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro AUC-ROC score: 0.7470094956221482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1985,  877],\n",
       "        [ 115,  259]]),\n",
       " {'0': {'precision': 0.9452380952380952,\n",
       "   'recall': 0.693570929419986,\n",
       "   'f1-score': 0.800080612656187,\n",
       "   'support': 2862},\n",
       "  '1': {'precision': 0.22799295774647887,\n",
       "   'recall': 0.6925133689839572,\n",
       "   'f1-score': 0.343046357615894,\n",
       "   'support': 374},\n",
       "  'accuracy': 0.6934487021013597,\n",
       "  'macro avg': {'precision': 0.586615526492287,\n",
       "   'recall': 0.6930421492019716,\n",
       "   'f1-score': 0.5715634851360405,\n",
       "   'support': 3236},\n",
       "  'weighted avg': {'precision': 0.8623426436244165,\n",
       "   'recall': 0.6934487021013597,\n",
       "   'f1-score': 0.7472589774939282,\n",
       "   'support': 3236}},\n",
       " 0.7470094956221482)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate saved model\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/ihm_model/\"\n",
    "PT_NAME_GLOB = \"ihm_mlp_spc10_e0_*.pt\"\n",
    "eval_model = network.IHMPreliminaryMLP(input_shape=(48, 42))\n",
    "\n",
    "model_pt = glob(os.path.join(SYN_DATA_SAVE_DIR, PT_NAME_GLOB))[0]\n",
    "model_data = torch.load(model_pt, map_location=torch.device(DEVICE))\n",
    "\n",
    "eval_model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "eval_model.to(DEVICE)\n",
    "eval_model.eval()\n",
    "\n",
    "# load eval set\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 256\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS) # pay attention to the test set used here\n",
    "\n",
    "report.run_classificatoin_report(eval_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª EHR Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ¿ Vanilla dataset distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Dataset Distillation intends to get dataset that is able to train a model within only 1 epoch\n",
    "\n",
    "# define hyper params\n",
    "NUM_OPTIM_IT = 1000\n",
    "EVAL_EVERY_NUM_IT = 10\n",
    "\n",
    "INIT_LR = 0.001\n",
    "STEP_SIZE = 0.001\n",
    "INIT_WEIGHTS_DISTR = \"kaiming\"\n",
    "BATCH_SIZE = 256\n",
    "NUM_SAMPLED_MODELS_TRAIN = 16\n",
    "NUM_SAMPLED_MODELS_EVAL = 4\n",
    "N_SAMPLES_PER_CLS = 10\n",
    "\n",
    "# checkpoints saving\n",
    "CHCKPNT_SAVE_DIR = \"./saved_data\"\n",
    "if not os.path.exists(CHCKPNT_SAVE_DIR):\n",
    "    os.makedirs(CHCKPNT_SAVE_DIR)\n",
    "\n",
    "# prepare dataloaders\n",
    "NUM_WORKERS = 8\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize random synth dataset\n",
    "ts_syn = torch.randn(size=(2*N_SAMPLES_PER_CLS, input_shape[0], input_shape[1]), dtype=torch.float, requires_grad=True, device=DEVICE) # device is ignored by far\n",
    "lab_syn = torch.tensor(np.array([np.ones(N_SAMPLES_PER_CLS)*i for i in (0, 1)]), dtype=torch.long, requires_grad=False, device=DEVICE).view(-1) # 1-D, length = episodes_per_cls * 2\n",
    "\n",
    "# initialize learning rate\n",
    "lr = torch.tensor([INIT_LR], dtype=torch.float, requires_grad=True, device=DEVICE) # make it learnable\n",
    "\n",
    "optimizer_ts = torch.optim.Adam([ts_syn], lr=STEP_SIZE)\n",
    "optimizer_lr = torch.optim.Adam([lr], lr=STEP_SIZE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# data used for plotting curves\n",
    "optim_losses = [] # optimization objective loss each iteration\n",
    "min_optim_loss = float('inf')\n",
    "syn_lrs = [] # synthetic lr\n",
    "eval_scores_train = [] # evaluation avg roc-auc score on train set\n",
    "eval_scores_test = [] # evaluation avg roc-auc score on test set\n",
    "\n",
    "# begin training steps\n",
    "pbar = tqdm(range(NUM_OPTIM_IT), desc=\"Distilling dataset using vanilla DD\")\n",
    "for it in pbar:\n",
    "    # get a minibatch of real training data\n",
    "    # Sample from class 0\n",
    "    ts_class_0, lab_class_0 = train_set.random_sample_from_class(n_samples=BATCH_SIZE//2, cls=0)\n",
    "    # ts_class_0, lab_class_0 = train_set.first_n_samples_from_class(n_samples=BATCH_SIZE//2, cls=0)\n",
    "    # Sample from class 1\n",
    "    ts_class_1, lab_class_1 = train_set.random_sample_from_class(n_samples=BATCH_SIZE//2, cls=1)\n",
    "    # ts_class_1, lab_class_1 = train_set.first_n_samples_from_class(n_samples=BATCH_SIZE//2, cls=1)\n",
    "    # Concatenate the time series data along the first dimension (batch size)\n",
    "    ts_real = torch.cat((ts_class_0, ts_class_1), dim=0).to(DEVICE)\n",
    "    # Concatenate the labels along the 0th dimension\n",
    "    lab_real = torch.cat((lab_class_0, lab_class_1), dim=0).to(DEVICE)\n",
    "    # print(ts_real.shape, lab_real.shape) # batch_size * num_time_steps * num_features\n",
    "\n",
    "    # evaluate the distilled data every `EVAL_EVERY_NUM_IT` iterations\n",
    "    if it % EVAL_EVERY_NUM_IT == 0:\n",
    "        print(f\"Optimization iteration {it} evaluation begins...\")\n",
    "        ts_syn_chckpnt = ts_syn.detach().clone()\n",
    "        # lab_syn are not learning objectives so just use it in-place\n",
    "        lr_chckpnt = lr.detach().clone()\n",
    "        # sample a batch of models\n",
    "        sampled_models = []\n",
    "        local_train_scores = []\n",
    "        local_test_scores = []\n",
    "        for j in range(NUM_SAMPLED_MODELS_EVAL):\n",
    "            torch.random.manual_seed(int(time.time() * 1000) % 100000) # random seed\n",
    "            # torch.random.manual_seed(42) # fixed seed\n",
    "            model = network.IHMPreliminary1DCNN(input_shape=input_shape, init_distr=INIT_WEIGHTS_DISTR).to(DEVICE)\n",
    "            sampled_models.append(model)\n",
    "        for model in sampled_models:\n",
    "            model.train()\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_chckpnt.item())\n",
    "            # train the models on synthetic set\n",
    "            pred_syn = model(ts_syn_chckpnt)\n",
    "            loss_syn = loss_fn(pred_syn, lab_syn)\n",
    "            optimizer.zero_grad()\n",
    "            loss_syn.backward()\n",
    "            optimizer.step()\n",
    "        for model in sampled_models:\n",
    "            # evaluate the models on both full train set and test set\n",
    "            train_auc_roc = report.compute_roc_auc_score(model, train_loader)\n",
    "            test_auc_roc = report.compute_roc_auc_score(model, test_loader)\n",
    "            local_train_scores.append(train_auc_roc)\n",
    "            local_test_scores.append(test_auc_roc)\n",
    "        eval_scores_train.append(sum(local_train_scores) / len(local_train_scores))\n",
    "        eval_scores_test.append(sum(local_test_scores) / len(local_test_scores))\n",
    "        print(f\"Optimization iteration {it}, eval score (train): {eval_scores_train[-1]:.4f}, eval score (test): {eval_scores_test[-1]:.4f}\")\n",
    "    \n",
    "    # sample a batch of models\n",
    "    sampled_models = []\n",
    "    for j in range(NUM_SAMPLED_MODELS_TRAIN):\n",
    "        torch.random.manual_seed(int(time.time() * 1000) % 100000) # random seed\n",
    "        # torch.random.manual_seed(42) # fixed seed\n",
    "        model = network.IHMPreliminary1DCNN(input_shape=input_shape, init_distr=INIT_WEIGHTS_DISTR).to(DEVICE)\n",
    "        sampled_models.append(model)\n",
    "        \n",
    "    optimizer_ts.zero_grad()\n",
    "    optimizer_lr.zero_grad()\n",
    "\n",
    "    losses = []\n",
    "    for model in sampled_models:\n",
    "        # Step 1: Train each sampled model on synthetic dataset\n",
    "        model.train()\n",
    "        pred_syn = model(ts_syn)\n",
    "        loss_syn = loss_fn(pred_syn, lab_syn)\n",
    "        \n",
    "        for m in model.modules():\n",
    "            param_names = []\n",
    "            new_params = []\n",
    "            for n, p in m.named_parameters(recurse=False): # n is the param's name alone instead of \"module.name\"\n",
    "                gp, = torch.autograd.grad(loss_syn, p, create_graph=True) # enabling higher-order derivatives\n",
    "                new_p = p - lr * gp\n",
    "                new_p.to(DEVICE)\n",
    "                param_names.append(n) # save them, to delete leaf params later in another enumeration\n",
    "                new_params.append(new_p) # save them, to reset non-leaf params later in another enumeration\n",
    "            for i, n in enumerate(param_names):\n",
    "                delattr(m, n)\n",
    "                setattr(m, n, new_params[i])\n",
    "\n",
    "        # Step 2: Evaluate the objective function on real training data\n",
    "        pred_real = model(ts_real)\n",
    "        loss_real = loss_fn(pred_real, lab_real)\n",
    "        losses.append(loss_real)\n",
    "\n",
    "        # Clear gradients for the next model\n",
    "        model.zero_grad()\n",
    "\n",
    "    # Check if params are swapped as non-leaves\n",
    "    # for model in sampled_models:\n",
    "    #     for m in model.modules():\n",
    "    #         for n, p in m.named_parameters(recurse=False): # name is the param's name alone instead of module.name\n",
    "    #             print(p.grad_fn)\n",
    "    \n",
    "    # Step 3: Update synthetic data and learnable learning rate\n",
    "    total_loss = sum(losses)\n",
    "    total_loss.backward()  # Compute gradients based on real data losses\n",
    "\n",
    "    # Update synthetic data and learning rate\n",
    "    # print(lr.grad) # shouldn't be none\n",
    "    # print(ts_syn.grad) # shouldn't be none\n",
    "    optimizer_ts.step()\n",
    "    optimizer_lr.step()\n",
    "\n",
    "    # Logging the progress\n",
    "    pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\",\n",
    "                      \"learnable lr\": f\"{lr.item()}\",\n",
    "                      })\n",
    "    optim_losses.append(total_loss.item())\n",
    "    syn_lrs.append(lr.item())\n",
    "\n",
    "    if total_loss.item() < min_optim_loss or it >= NUM_OPTIM_IT - 1: # save checkpoint\n",
    "        min_optim_loss = total_loss.item()\n",
    "        print(f\"New best! Saving checkpoint iteration {it}...\")\n",
    "        checkpoint = {\n",
    "            \"it\": it,\n",
    "            \"ts_syn\": ts_syn.detach().clone(),\n",
    "            \"lab_syn\": lab_syn.detach().clone(),\n",
    "            'optim_losses': optim_losses,\n",
    "            'syn_lrs': syn_lrs,\n",
    "            'eval_scores_train': eval_scores_train,\n",
    "            'eval_scores_test': eval_scores_test,\n",
    "        }\n",
    "        # Save checkpoint\n",
    "        torch.save(checkpoint, os.path.join(CHCKPNT_SAVE_DIR, 'distillation_checkpoint.pth'))\n",
    "        print(f\"Checkpoint at iteration {it} saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒˆ Distill by matching gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa20db6c28514be88a0cb33d7429078a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distill with matching loss\n",
    "\n",
    "MODEL = [\"1dcnn\", \"mlp\"][1]\n",
    "\n",
    "if MODEL == \"1dcnn\":\n",
    "    get_model = network.IHMPreliminary1DCNN\n",
    "else:\n",
    "    get_model = network.IHMPreliminaryMLP\n",
    "# define hyper params\n",
    "TRAIN_IT = 100\n",
    "EVAL_EVERY_NUM_IT = -1\n",
    "EVAL_NUM_EPOCHS = 50\n",
    "NUM_SAMPLED_MODELS_EVAL = 4\n",
    "LR_DATA = 0.01\n",
    "LR_NET = 0.01\n",
    "NUM_OUTER_LOOP = 10\n",
    "NUM_INNER_LOOP = 50\n",
    "REAL_BATCH_SIZE = 256\n",
    "SYN_BATCH_SIZE = 256\n",
    "\n",
    "N_SAMPLES_PER_CLS = 10\n",
    "INIT_WEIGHTS_DISTR = None\n",
    "\n",
    "# prepare dataloaders\n",
    "NUM_WORKERS = 8\n",
    "train_loader = DataLoader(train_set, REAL_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_set, REAL_BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "# initialize synthetic data\n",
    "episodes_syn = torch.randn(size=(2*N_SAMPLES_PER_CLS, input_shape[0], input_shape[1]), dtype=torch.float, requires_grad=True, device=DEVICE)\n",
    "labels_syn = torch.tensor(np.array([np.ones(N_SAMPLES_PER_CLS)*i for i in (0, 1)]), dtype=torch.long, requires_grad=False, device=DEVICE).view(-1) # 1D, length = episodes_per_cls * 2\n",
    "\n",
    "# define training optimizers and criterion\n",
    "optimizer_ts = torch.optim.SGD([episodes_syn,], lr=LR_DATA, momentum=0.5) # optimizer for synthetic data\n",
    "optimizer_ts.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Ready for training\")\n",
    "\n",
    "# data for plotting curves\n",
    "eval_scores_train = [] # evaluation avg roc-auc score on train set\n",
    "eval_scores_test = [] # evaluation avg roc-auc score on test set\n",
    "avg_losses = []\n",
    "min_avg_loss = float('inf')\n",
    "\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/dd/\"\n",
    "if not os.path.exists(SYN_DATA_SAVE_DIR):\n",
    "    os.makedirs(SYN_DATA_SAVE_DIR)\n",
    "pbar = tqdm(range(TRAIN_IT), desc=\"Training iteration\")\n",
    "for it in pbar:\n",
    "\n",
    "    # evaluate the distilled data every `EVAL_EVERY_NUM_IT` iterations\n",
    "    if EVAL_EVERY_NUM_IT > 0 and it % EVAL_EVERY_NUM_IT == 0:\n",
    "        print(f\"Distillation iteration {it} evaluation begins...\")\n",
    "        episodes_syn_chckpnt = copy.deepcopy(episodes_syn.detach())\n",
    "        syn_dataset = dataset.TensorDataset(episodes_syn_chckpnt, copy.deepcopy(labels_syn)) # lab_syn are not learning objectives so just use it in-place\n",
    "        syn_loader = DataLoader(syn_dataset, SYN_BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        # sample a batch of models\n",
    "        sampled_models = []\n",
    "        local_train_scores = []\n",
    "        local_test_scores = []\n",
    "        for j in range(NUM_SAMPLED_MODELS_EVAL):\n",
    "            torch.random.manual_seed(int(time.time() * 1000) % 100000) # random seed\n",
    "            # torch.random.manual_seed(42) # fixed seed\n",
    "            model = get_model(input_shape=input_shape, init_distr=INIT_WEIGHTS_DISTR).to(DEVICE)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=LR_NET)\n",
    "            sampled_models.append(model)\n",
    "        for model in sampled_models:\n",
    "            # update network\n",
    "            for e in range(EVAL_NUM_EPOCHS):\n",
    "                train.epoch(\"train\", syn_loader, model, criterion, optimizer=optimizer)\n",
    "        \n",
    "        print(f\"Training {NUM_SAMPLED_MODELS_EVAL} models for evaluation done. Generating classification reports...\")\n",
    "        sub_pbar = tqdm(range(NUM_SAMPLED_MODELS_EVAL), desc=\"Generating classification reports for each model...\",leave=False)\n",
    "        for idx in sub_pbar:\n",
    "            model = sampled_models[idx]\n",
    "            # evaluate the models on both full train set and test set\n",
    "            train_auc_roc = report.compute_roc_auc_score(model, train_loader)\n",
    "            test_auc_roc = report.compute_roc_auc_score(model, test_loader)\n",
    "            local_train_scores.append(train_auc_roc)\n",
    "            local_test_scores.append(test_auc_roc)\n",
    "        eval_scores_train.append(sum(local_train_scores) / len(local_train_scores))\n",
    "        eval_scores_test.append(sum(local_test_scores) / len(local_test_scores))\n",
    "        print(f\"Optimization iteration {it}, eval score (train): {eval_scores_train[-1]:.4f}, eval score (test): {eval_scores_test[-1]:.4f}\")\n",
    "    \n",
    "\n",
    "    # Train synthetic data\n",
    "    torch.random.manual_seed(int(time.time() * 1000) % 100000) # random init network\n",
    "    net = get_model(input_shape=input_shape, init_distr=INIT_WEIGHTS_DISTR).to(DEVICE)\n",
    "    net.train()\n",
    "    net_params = list(net.parameters())\n",
    "\n",
    "    optimizer_net = torch.optim.SGD(net.parameters(), lr=LR_NET)\n",
    "    optimizer_net.zero_grad()\n",
    "    loss_avg = 0\n",
    "\n",
    "    for ol in range(NUM_OUTER_LOOP):\n",
    "        # update synthetic data\n",
    "        loss = torch.tensor(0.0).to(DEVICE)\n",
    "        for cls in (0, 1):\n",
    "            ts_real, lab_real = train_set.random_sample_from_class(n_samples=REAL_BATCH_SIZE, cls=cls, no_duplicate=True)\n",
    "            ts_real, lab_real = ts_real.to(DEVICE), lab_real.to(DEVICE)\n",
    "            ts_syn = episodes_syn[cls*N_SAMPLES_PER_CLS: (cls+1)*N_SAMPLES_PER_CLS]\n",
    "            lab_syn = labels_syn[cls*N_SAMPLES_PER_CLS: (cls+1)*N_SAMPLES_PER_CLS]\n",
    "\n",
    "            out_real = net(ts_real)\n",
    "            loss_real = criterion(out_real, lab_real)\n",
    "            grad_real = torch.autograd.grad(loss_real, net_params)\n",
    "            grad_real = [_.detach().clone() for _ in grad_real]\n",
    "\n",
    "            out_syn = net(ts_syn)\n",
    "            loss_syn = criterion(out_syn, lab_syn)\n",
    "            grad_syn = torch.autograd.grad(loss_syn, net_params, create_graph=True) # create_graph: will be used to compute higher-order derivatives\n",
    "\n",
    "            # compute gradient matching loss, here using MSE, instead of the one proposed in DCwMG because it's too complicated\n",
    "            # dis = torch.tensor(0.0).to(DEVICE)\n",
    "            grad_real_vec = []\n",
    "            grad_syn_vec = []\n",
    "            for ig in range(len(grad_real)):\n",
    "                grad_real_vec.append(grad_real[ig].reshape((-1)))\n",
    "                grad_syn_vec.append(grad_syn[ig].reshape((-1)))\n",
    "            grad_real_vec = torch.cat(grad_real_vec, dim=0)\n",
    "            grad_syn_vec = torch.cat(grad_syn_vec, dim=0)\n",
    "            dis = torch.sum((grad_syn_vec - grad_real_vec)**2)\n",
    "\n",
    "            loss += dis\n",
    "        \n",
    "        optimizer_ts.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ts.step()\n",
    "        loss_avg += loss.item()\n",
    "\n",
    "        if ol == NUM_OUTER_LOOP - 1:\n",
    "            break\n",
    "\n",
    "        # update network\n",
    "        episodes_syn_train, labels_syn_train = copy.deepcopy(episodes_syn.detach()), copy.deepcopy(labels_syn.detach())  # avoid any unaware modification\n",
    "        syn_dataset = dataset.TensorDataset(episodes_syn_train, labels_syn_train)\n",
    "        syn_loader = DataLoader(syn_dataset, SYN_BATCH_SIZE, shuffle=True)\n",
    "        for il in range(NUM_INNER_LOOP):\n",
    "            train.epoch(\"train\", syn_loader, net, criterion, optimizer=optimizer_net)\n",
    "\n",
    "    loss_avg /= (2 * NUM_OUTER_LOOP)\n",
    "    pbar.set_postfix({\"avg loss\": f\"{loss_avg:.4f}\",\n",
    "                      })\n",
    "    avg_losses.append(loss_avg)\n",
    "    if loss_avg < min_avg_loss:\n",
    "        min_avg_loss = loss_avg\n",
    "\n",
    "    torch.save({\"data\": (copy.deepcopy(episodes_syn.detach().cpu()), copy.deepcopy(labels_syn.detach().cpu())),\n",
    "                \"size\": (2*N_SAMPLES_PER_CLS, input_shape[0], input_shape[1]),\n",
    "                },\n",
    "            os.path.join(SYN_DATA_SAVE_DIR, f\"mg_{MODEL}_it{it}_avgl{loss_avg:.4f}.pt\")\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤”ï¸ Evaluate distilled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simultaneously train 2 models on distilled dataset and original dataset, compare performance\n",
    "N_SAMPLES_PER_CLS = 10\n",
    "\n",
    "REAL_BATCH_SIZE = 256\n",
    "SYN_BATCH_SIZE = 2*N_SAMPLES_PER_CLS\n",
    "\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/dd/\"\n",
    "SYN_DATA_GLOB = \"md_it47_*.pt\"\n",
    "syn_pt_path = glob(os.path.join(SYN_DATA_SAVE_DIR, SYN_DATA_GLOB))[0]\n",
    "syn_data = torch.load(syn_pt_path, map_location=torch.device(DEVICE))\n",
    "syn_ts, syn_lab = syn_data[\"data\"]\n",
    "syn_ts = syn_ts.to(DEVICE)\n",
    "syn_lab = syn_lab.to(DEVICE)\n",
    "syn_set = dataset.TensorDataset(syn_ts, syn_lab)\n",
    "syn_loader = DataLoader(syn_set, 2*N_SAMPLES_PER_CLS)\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "real_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "real_loader = DataLoader(real_set, REAL_BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "model_syn = network.IHMPreliminary1DCNN().to(DEVICE)\n",
    "model_real = network.IHMPreliminary1DCNN().to(DEVICE)\n",
    "\n",
    "train_epoch = 100\n",
    "lr = 0.01\n",
    "\n",
    "optimizer_syn = torch.optim.SGD(model_syn.parameters(), lr)\n",
    "optimizer_syn.zero_grad()\n",
    "optimizer_real = torch.optim.SGD(model_real.parameters(), lr)\n",
    "optimizer_real.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_syn.train()\n",
    "model_real.train()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(train_epoch), desc=\"Evaluating synthetic dataset\")\n",
    "for e in pbar:\n",
    "    train.epoch(\"train\", syn_loader, model_syn, criterion, optimizer_syn, DEVICE)\n",
    "    syn_loss, syn_acc = train.epoch(\"test\", real_loader, model_syn, criterion, optimizer_syn, DEVICE)\n",
    "    real_loss, real_acc = train.epoch(\"train\", real_loader, model_real, criterion, optimizer_real, DEVICE)\n",
    "    pbar.set_description(f\"Evaluating epoch {e}\\nsyn loss = {syn_loss}, syn acc = {syn_acc}\\nreal loss = {real_loss}, real acc = {real_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on saved distilled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a53c74749c41c3b01aa57476bb84e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training on distilled dataset (size=20):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     52\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mepoch(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, syn_loader, model, loss_fn, optimizer, device\u001b[39m=\u001b[39mDEVICE) \u001b[39m# attention: using sampled dataset\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mepoch(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, test_loader, model, loss_fn, device\u001b[39m=\u001b[39;49mDEVICE)\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m train_loss \u001b[39m<\u001b[39m min_loss:\n\u001b[1;32m     55\u001b[0m         filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdihm_\u001b[39m\u001b[39m{\u001b[39;00mMODEL\u001b[39m}\u001b[39;00m\u001b[39m_spc\u001b[39m\u001b[39m{\u001b[39;00mN_SAMPLES_PER_CLS\u001b[39m}\u001b[39;00m\u001b[39m_e\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m_trl\u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_tel\u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m/project/ruishanl_1185/EHR-Distillation/utils/train.py:16\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(mode, dataloader, net, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     net\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, datum \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     17\u001b[0m     feat \u001b[39m=\u001b[39m datum[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     lab \u001b[39m=\u001b[39m datum[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/.conda/envs/playground/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model on saved distilled data\n",
    "N_SAMPLES_PER_CLS = 10\n",
    "\n",
    "SYN_BATCH_SIZE = 2*N_SAMPLES_PER_CLS\n",
    "\n",
    "BATCH_SIZE=256\n",
    "\n",
    "MODEL_PT_SAVE_DIR = \"./saved_data/ihm_model/\"\n",
    "SYN_DATA_SAVE_DIR = \"./saved_data/dd/\"\n",
    "SYN_DATA_GLOB = \"md_it47_*.pt\"\n",
    "syn_pt_path = glob(os.path.join(SYN_DATA_SAVE_DIR, SYN_DATA_GLOB))[0]\n",
    "syn_data = torch.load(syn_pt_path, map_location=torch.device(DEVICE))\n",
    "syn_ts, syn_lab = syn_data[\"data\"]\n",
    "input_shape = syn_data[\"size\"][1:]\n",
    "syn_ts = syn_ts.detach().to(DEVICE)\n",
    "syn_lab = syn_lab.to(DEVICE)\n",
    "syn_set = dataset.TensorDataset(syn_ts, syn_lab)\n",
    "\n",
    "# define dataloaders\n",
    "NUM_WORKERS = 8\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "syn_loader = DataLoader(syn_set, batch_size=SYN_BATCH_SIZE)\n",
    "\n",
    "# check if data is as expected\n",
    "# pos_cnt = 0\n",
    "# for (x, y) in syn_set:\n",
    "#     print(x, y)\n",
    "#     if y.item() == 1:\n",
    "#         pos_cnt += 1\n",
    "# print(pos_cnt)\n",
    "\n",
    "MODEL = [\"1dcnn\", \"mlp\"][0]\n",
    "\n",
    "if MODEL == \"1dcnn\":\n",
    "    model = network.IHMPreliminary1DCNN(input_shape=input_shape).to(DEVICE)\n",
    "else:\n",
    "    model = network.IHMPreliminaryMLP(input_shape=input_shape).to(DEVICE)\n",
    "\n",
    "EPOCH = 100\n",
    "LR = 0.001\n",
    "WD = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "optimizer.zero_grad()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "pbar = tqdm(range(EPOCH), desc=f\"Training on distilled dataset (size={2*N_SAMPLES_PER_CLS})\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", syn_loader, model, loss_fn, optimizer, device=DEVICE) # attention: using sampled dataset\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, loss_fn, device=DEVICE)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'dihm_{MODEL}_spc{N_SAMPLES_PER_CLS}_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(MODEL_PT_SAVE_DIR, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(MODEL_PT_SAVE_DIR) if f.startswith(f'dihm_{MODEL}_spc{N_SAMPLES_PER_CLS}_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(MODEL_PT_SAVE_DIR, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_postfix({\"train loss\": f\"{train_loss:.4f}\",\n",
    "                      \"train acc\": f\"{train_acc*100:.2f}%\",\n",
    "                      \"test loss\": f\"{test_loss:.4f}\",\n",
    "                      \"test acc\": f\"{test_acc*100:.2f}%\",\n",
    "                      })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
