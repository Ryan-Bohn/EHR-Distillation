{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary experiment - distilled dataset for in-hospital mortality prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Dependencies.\n",
    "\n",
    "For cuda, define `device` that'll be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Custom libs. **Always re-run the following code block after modifying `utils`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.report' from '/Users/delphynium/Documents/research projects/EHR-Distillation/utils/report.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import preprocess, dataset, network, train, report\n",
    "importlib.reload(preprocess)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(network)\n",
    "importlib.reload(train)\n",
    "importlib.reload(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Compute statistics that'll be used for imputation and dataloading.\n",
    "\n",
    "Statistics can be load from saved pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_saved = True\n",
    "stat_pkl_dir = \"./saved_data/stats/\"\n",
    "if not os.path.exists(stat_pkl_dir):\n",
    "    os.makedirs(stat_pkl_dir)\n",
    "\n",
    "categorical_numcls = {  # how many classes are there for categorical classes\n",
    "    \"capillary_refill_rate\": 2,\n",
    "    \"glascow_coma_scale_eye_opening\": 4,\n",
    "    \"glascow_coma_scale_motor_response\": 6,\n",
    "    \"glascow_coma_scale_total\": 13,\n",
    "    \"glascow_coma_scale_verbal_response\": 5,\n",
    "}\n",
    "\n",
    "pkl_path = os.path.join(stat_pkl_dir, \"ihm_preliminary.pkl\")\n",
    "if os.path.exists(pkl_path) and load_from_saved:\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        continuous_avgs_train, continuous_stds_train, categorical_modes_train = pickle.load(f)\n",
    "else:\n",
    "    continuous_avgs_train, continuous_stds_train, categorical_modes_train =  preprocess.compute_feature_statistics(\n",
    "        ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/train/\",\n",
    "        feature_dict=preprocess.mimic3_benchmark_variable_dict\n",
    "        )\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump((continuous_avgs_train, continuous_stds_train, categorical_modes_train), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean all data by resampling, imputating and masking.\n",
    "\n",
    "**Running this block once is enough.**\n",
    "(Cleaned data will be saved at ./data/mimic3/ihm_preliminary/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.preprocess_ihm_timeseries_files(\n",
    "    ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/train/\",\n",
    "    output_dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    feature_dict=preprocess.mimic3_benchmark_variable_dict,\n",
    "    normal_value_dict=continuous_avgs_train|categorical_modes_train\n",
    "    )\n",
    "preprocess.preprocess_ihm_timeseries_files(\n",
    "    ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/test/\",\n",
    "    output_dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    feature_dict=preprocess.mimic3_benchmark_variable_dict,\n",
    "    normal_value_dict=continuous_avgs_train|categorical_modes_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’­ Evaluating model capacity on training objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and evaluation set for IHM objective.\n",
    "\n",
    "Apply mask / balance to the dataset here.\n",
    "\n",
    "You may have to re-run this block after modifying dataloader-related codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in the dataset: \n",
      "(tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -3.8303e-16],\n",
      "        [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -3.8303e-16],\n",
      "        [ 1.0000e+00,  0.0000e+00,  3.9972e-02,  ...,  5.6418e-02,\n",
      "          1.4145e+00,  4.7770e-02],\n",
      "        ...,\n",
      "        [ 1.0000e+00,  0.0000e+00,  2.6577e-02,  ...,  8.5097e-02,\n",
      "          1.4145e+00,  9.9521e-02],\n",
      "        [ 1.0000e+00,  0.0000e+00,  1.0695e-01,  ..., -1.8149e-02,\n",
      "          1.4145e+00,  9.9521e-02],\n",
      "        [ 1.0000e+00,  0.0000e+00,  2.2111e-02,  ..., -1.8149e-02,\n",
      "          1.4145e+00,  9.9521e-02]]), tensor(0))\n",
      "Feature tensor shape: torch.Size([48, 42])\n",
      "First item in the dataset: \n",
      "(tensor([[ 1.0000,  0.0000, -0.0583,  ..., -0.0835,  0.0000,  0.0607],\n",
      "        [ 1.0000,  0.0000, -0.0449,  ...,  0.0197,  0.0000,  0.0607],\n",
      "        [ 1.0000,  0.0000, -0.0806,  ...,  0.0507,  0.0000,  0.0564],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000, -0.0404,  ..., -0.0698,  0.4754,  0.0693],\n",
      "        [ 1.0000,  0.0000, -0.0315,  ..., -0.0698,  0.4754,  0.0693],\n",
      "        [ 1.0000,  0.0000, -0.0315,  ..., -0.0698,  0.4754,  0.0693]]), tensor(0))\n",
      "Feature tensor shape: torch.Size([48, 42])\n",
      "Input tensor shape: torch.Size([48, 42])\n"
     ]
    }
   ],
   "source": [
    "# Pay attention to balance and mask settings\n",
    "\n",
    "train_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "print(f\"First item in the dataset: \\n{train_set[0]}\")\n",
    "print(f\"Feature tensor shape: {train_set[0][0].shape}\")\n",
    "\n",
    "test_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    dstype=\"test\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "print(f\"First item in the dataset: \\n{test_set[0]}\")\n",
    "print(f\"Feature tensor shape: {test_set[0][0].shape}\")\n",
    "\n",
    "input_shape = train_set[0][0].shape\n",
    "print(f\"Input tensor shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the label distribution in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0_cnt = 0\n",
    "label_1_cnt = 1\n",
    "for _, label in train_set:\n",
    "    if label > 0.5:\n",
    "        label_1_cnt += 1\n",
    "    else:\n",
    "        label_0_cnt += 1\n",
    "print(f\"Label 0 ratio: {label_0_cnt / (label_0_cnt + label_1_cnt)}\")\n",
    "print(f\"Label 1 ratio: {label_1_cnt / (label_0_cnt + label_1_cnt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper params\n",
    "ihm_epoch = 100\n",
    "ihm_batch_size = 256\n",
    "ihm_lr = 0.001\n",
    "ihm_wd = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an 1D CNN and save the best performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1D CNN\n",
    "num_workers = 8\n",
    "pkl_save_dir = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "\n",
    "train_loader = DataLoader(train_set, ihm_batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=num_workers)\n",
    "\n",
    "model = network.IHMPreliminary1DCNN(input_shape=input_shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(ihm_epoch), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", train_loader, model, criterion, optimizer, device=device)\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, criterion, device=device)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_1dcnn_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(pkl_save_dir, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(pkl_save_dir) if f.startswith(f'ihm_1dcnn_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(pkl_save_dir, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_description(f\"Training on original task, epoch {e}\\ntrain loss = {train_loss}, train acc = {train_acc}\\ntest loss = {test_loss}, test acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an MLP and save the best performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP\n",
    "num_workers = 8\n",
    "pkl_save_dir = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "\n",
    "train_loader = DataLoader(train_set, ihm_batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=num_workers)\n",
    "\n",
    "model = network.IHMPreliminaryMLP(input_shape=input_shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(ihm_epoch), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, criterion, device=device)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_mlp_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(pkl_save_dir, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(pkl_save_dir) if f.startswith(f'ihm_mlp_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(pkl_save_dir, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_description(f\"Training on original task, epoch {e}\\ntrain loss = {train_loss}, train acc = {train_acc}\\ntest loss = {test_loss}, test acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a saved model and evaluate on evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate saved model\n",
    "pt_save_dir = \"./saved_data/ihm_model/\"\n",
    "model_name = \"ihm_1dcnn_*_balanced.pt\"\n",
    "eval_model = network.IHMPreliminary1DCNN(input_shape=(48, 42))\n",
    "\n",
    "model_pt = glob(os.path.join(pt_save_dir, model_name))[0]\n",
    "model_data = torch.load(model_pt, map_location=torch.device(device))\n",
    "\n",
    "eval_model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "eval_model.to(device)\n",
    "eval_model.eval()\n",
    "\n",
    "num_workers = 8\n",
    "balanced_test_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    dstype=\"test\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=True,\n",
    "    mask=False,\n",
    "    )\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=num_workers) # pay attention to the test set used here\n",
    "\n",
    "report.run_classificatoin_report(eval_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: train the model with randomly sampled subset of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1D CNN with random sampled 20 datapoints\n",
    "# TODO\n",
    "num_workers = 8\n",
    "pkl_save_dir = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "\n",
    "num_workers = 8\n",
    "# define real train and test set\n",
    "# adjust balance and mask settings for datasets here\n",
    "train_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "input_shape = train_set[0][0].shape\n",
    "# prepare loaders\n",
    "train_loader = DataLoader(train_set, 256, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Sample from class 0\n",
    "ts_class_0, lab_class_0 = train_set.random_sample_from_class(n_samples=10, cls=0)\n",
    "# ts_class_0, lab_class_0 = train_set.first_n_samples_from_class(n_samples=batch_size//2, cls=0)\n",
    "# Sample from class 1\n",
    "ts_class_1, lab_class_1 = train_set.random_sample_from_class(n_samples=10, cls=1)\n",
    "# ts_class_1, lab_class_1 = train_set.first_n_samples_from_class(n_samples=batch_size//2, cls=1)\n",
    "# Concatenate the time series data along the first dimension (batch size)\n",
    "ts_real = torch.cat((ts_class_0, ts_class_1), dim=0).to(device)\n",
    "# Concatenate the labels along the 0th dimension\n",
    "lab_real = torch.cat((lab_class_0, lab_class_1), dim=0).to(device)\n",
    "# print(ts_real.shape, lab_real.shape) # batch_size * num_time_steps * num_features\n",
    "\n",
    "model = network.IHMPreliminary1DCNN(input_shape=input_shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(100), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    # train the models on synthetic set\n",
    "    pred_real = model(ts_real)\n",
    "    loss_real = loss_fn(pred_real, lab_real)\n",
    "    optimizer.zero_grad()\n",
    "    loss_real.backward()\n",
    "    optimizer.step()\n",
    "    if e % 10 == 0:\n",
    "        train_auc_roc = report.compute_roc_auc_score(model, train_loader)\n",
    "        print(train_auc_roc)\n",
    "    # if train_loss < min_loss:\n",
    "    #     filename = f'ihm_1dcnn_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "    #     file_path = os.path.join(pkl_save_dir, filename)\n",
    "\n",
    "    #     # Remove the previous checkpoint if it exists\n",
    "    #     existing_pts = [f for f in os.listdir(pkl_save_dir) if f.startswith(f'ihm_1dcnn_e{e}_') and f.endswith('.pt')]\n",
    "    #     for f in existing_pts:\n",
    "    #         os.remove(os.path.join(pkl_save_dir, f))\n",
    "    #     torch.save({\n",
    "    #             'epoch': e,\n",
    "    #             'model_state_dict': model.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #             'loss': train_loss,\n",
    "    #         }, file_path)\n",
    "    \n",
    "    pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\",\n",
    "                      \"learnable lr\": f\"{lr.item()}\",\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 EHR Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Vanilla dataset distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4f01265cbd4d5882a1b13bea19c742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization iteration 0 evaluation begins...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/delphynium/Documents/research projects/EHR-Distillation/ihm_preliminary.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/delphynium/Documents/research%20projects/EHR-Distillation/ihm_preliminary.ipynb#X32sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/delphynium/Documents/research%20projects/EHR-Distillation/ihm_preliminary.ipynb#X32sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m sampled_models:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/delphynium/Documents/research%20projects/EHR-Distillation/ihm_preliminary.ipynb#X32sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39m# evaluate the models on both full train set and test set\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/delphynium/Documents/research%20projects/EHR-Distillation/ihm_preliminary.ipynb#X32sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     train_auc_roc \u001b[39m=\u001b[39m report\u001b[39m.\u001b[39mcompute_roc_auc_score(model, train_loader)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/delphynium/Documents/research%20projects/EHR-Distillation/ihm_preliminary.ipynb#X32sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     test_auc_roc \u001b[39m=\u001b[39m report\u001b[39m.\u001b[39mcompute_roc_auc_score(model, test_loader)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/delphynium/Documents/research%20projects/EHR-Distillation/ihm_preliminary.ipynb#X32sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     local_train_scores\u001b[39m.\u001b[39mappend(train_auc_roc)\n",
      "File \u001b[0;32m~/Documents/research projects/EHR-Distillation/utils/report.py:23\u001b[0m, in \u001b[0;36mcompute_roc_auc_score\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m model_scores \u001b[39m=\u001b[39m []  \u001b[39m# To store the softmax scores for each class\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     24\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     25\u001b[0m         softmax_scores \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Apply softmax to convert logits to probabilities\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m w\u001b[39m.\u001b[39mstart()\n\u001b[1;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Popen(\u001b[39mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39mget_context()\u001b[39m.\u001b[39mProcess\u001b[39m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/playground/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39mwrite(fp\u001b[39m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vanilla Dataset Distillation intends to get dataset that is able to train a model within only 1 epoch\n",
    "\n",
    "# define hyper params\n",
    "num_optim_it = 1000\n",
    "eval_every_num_it = 10\n",
    "\n",
    "init_lr = 0.001\n",
    "step_size = 0.001\n",
    "init_weights_distr = \"kaiming\"\n",
    "batch_size = 256\n",
    "num_sampled_models_train = 16\n",
    "num_sampled_models_eval = 4\n",
    "ts_per_cls = 10\n",
    "\n",
    "# checkpoints saving\n",
    "chckpnt_save_dir = \"./saved_data\"\n",
    "if not os.path.exists(chckpnt_save_dir):\n",
    "    os.makedirs(chckpnt_save_dir)\n",
    "\n",
    "num_workers = 8\n",
    "# define real train and test set\n",
    "# adjust balance and mask settings for datasets here\n",
    "train_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "test_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    dstype=\"test\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "input_shape = train_set[0][0].shape\n",
    "# prepare loaders\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size, num_workers=num_workers)\n",
    "\n",
    "# initialize random synth dataset\n",
    "ts_syn = torch.randn(size=(2*ts_per_cls, input_shape[0], input_shape[1]), dtype=torch.float, requires_grad=True, device=device) # device is ignored by far\n",
    "lab_syn = torch.tensor(np.array([np.ones(ts_per_cls)*i for i in (0, 1)]), dtype=torch.long, requires_grad=False, device=device).view(-1) # 1-D, length = episodes_per_cls * 2\n",
    "\n",
    "# initialize learning rate\n",
    "lr = torch.tensor([init_lr], dtype=torch.float, requires_grad=True, device=device) # make it learnable\n",
    "\n",
    "optimizer_ts = torch.optim.Adam([ts_syn], lr=step_size)\n",
    "optimizer_lr = torch.optim.Adam([lr], lr=step_size)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# data used for plotting curves\n",
    "optim_losses = [] # optimization objective loss each iteration\n",
    "min_optim_loss = float('inf')\n",
    "syn_lrs = [] # synthetic lr\n",
    "eval_scores_train = [] # evaluation avg roc-auc score on train set\n",
    "eval_scores_test = [] # evaluation avg roc-auc score on test set\n",
    "\n",
    "# begin training steps\n",
    "pbar = tqdm(range(num_optim_it), desc=\"Training iteration\")\n",
    "for it in pbar:\n",
    "    # get a minibatch of real training data\n",
    "    # Sample from class 0\n",
    "    ts_class_0, lab_class_0 = train_set.random_sample_from_class(n_samples=batch_size//2, cls=0)\n",
    "    # ts_class_0, lab_class_0 = train_set.first_n_samples_from_class(n_samples=batch_size//2, cls=0)\n",
    "    # Sample from class 1\n",
    "    ts_class_1, lab_class_1 = train_set.random_sample_from_class(n_samples=batch_size//2, cls=1)\n",
    "    # ts_class_1, lab_class_1 = train_set.first_n_samples_from_class(n_samples=batch_size//2, cls=1)\n",
    "    # Concatenate the time series data along the first dimension (batch size)\n",
    "    ts_real = torch.cat((ts_class_0, ts_class_1), dim=0).to(device)\n",
    "    # Concatenate the labels along the 0th dimension\n",
    "    lab_real = torch.cat((lab_class_0, lab_class_1), dim=0).to(device)\n",
    "    # print(ts_real.shape, lab_real.shape) # batch_size * num_time_steps * num_features\n",
    "\n",
    "    # evaluate the distilled data every eval_every_num_it iterations\n",
    "    if it % eval_every_num_it == 0:\n",
    "        print(f\"Optimization iteration {it} evaluation begins...\")\n",
    "        ts_syn_chckpnt = ts_syn.detach().clone()\n",
    "        # lab_syn are not learning objectives so just use it in-place\n",
    "        lr_chckpnt = lr.detach().clone()\n",
    "        # sample a batch of models\n",
    "        sampled_models = []\n",
    "        local_train_scores = []\n",
    "        local_test_scores = []\n",
    "        for j in range(num_sampled_models_eval):\n",
    "            torch.random.manual_seed(int(time.time() * 1000) % 100000) # random seed\n",
    "            # torch.random.manual_seed(42) # fixed seed\n",
    "            model = network.IHMPreliminary1DCNN(input_shape=input_shape, init_distr=init_weights_distr).to(device)\n",
    "            sampled_models.append(model)\n",
    "        for model in sampled_models:\n",
    "            model.train()\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_chckpnt.item())\n",
    "            # train the models on synthetic set\n",
    "            pred_syn = model(ts_syn_chckpnt)\n",
    "            loss_syn = loss_fn(pred_syn, lab_syn)\n",
    "            optimizer.zero_grad()\n",
    "            loss_syn.backward()\n",
    "            optimizer.step()\n",
    "        for model in sampled_models:\n",
    "            # evaluate the models on both full train set and test set\n",
    "            train_auc_roc = report.compute_roc_auc_score(model, train_loader)\n",
    "            test_auc_roc = report.compute_roc_auc_score(model, test_loader)\n",
    "            local_train_scores.append(train_auc_roc)\n",
    "            local_test_scores.append(test_auc_roc)\n",
    "        eval_scores_train.append(sum(local_train_scores) / len(local_train_scores))\n",
    "        eval_scores_test.append(sum(local_test_scores) / len(local_test_scores))\n",
    "        print(f\"Optimization iteration {it}, eval score (train): {eval_scores_train[-1]:.4f}, eval score (test): {eval_scores_test[-1]:.4f}\")\n",
    "    # sample a batch of models\n",
    "    sampled_models = []\n",
    "    for j in range(num_sampled_models_train):\n",
    "        torch.random.manual_seed(int(time.time() * 1000) % 100000) # random seed\n",
    "        # torch.random.manual_seed(42) # fixed seed\n",
    "        model = network.IHMPreliminary1DCNN(input_shape=input_shape, init_distr=init_weights_distr).to(device)\n",
    "        sampled_models.append(model)\n",
    "        \n",
    "    optimizer_ts.zero_grad()\n",
    "    optimizer_lr.zero_grad()\n",
    "\n",
    "    losses = []\n",
    "    for model in sampled_models:\n",
    "        # Step 1: Train each sampled model on synthetic dataset\n",
    "        model.train()\n",
    "        pred_syn = model(ts_syn)\n",
    "        loss_syn = loss_fn(pred_syn, lab_syn)\n",
    "        \n",
    "        for m in model.modules():\n",
    "            param_names = []\n",
    "            new_params = []\n",
    "            for n, p in m.named_parameters(recurse=False): # n is the param's name alone instead of \"module.name\"\n",
    "                gp, = torch.autograd.grad(loss_syn, p, create_graph=True) # enabling higher-order derivatives\n",
    "                new_p = p - lr * gp\n",
    "                new_p.to(device)\n",
    "                param_names.append(n) # save them, to delete leaf params later in another enumeration\n",
    "                new_params.append(new_p) # save them, to reset non-leaf params later in another enumeration\n",
    "            for i, n in enumerate(param_names):\n",
    "                delattr(m, n)\n",
    "                setattr(m, n, new_params[i])\n",
    "\n",
    "        # Step 2: Evaluate the objective function on real training data\n",
    "        pred_real = model(ts_real)\n",
    "        loss_real = loss_fn(pred_real, lab_real)\n",
    "        losses.append(loss_real)\n",
    "\n",
    "        # Clear gradients for the next model\n",
    "        model.zero_grad()\n",
    "\n",
    "    # Check if params are swapped as non-leaves\n",
    "    for model in sampled_models:\n",
    "        for m in model.modules():\n",
    "            for n, p in m.named_parameters(recurse=False): # name is the param's name alone instead of module.name\n",
    "                print(p.grad_fn)\n",
    "    \n",
    "    # Step 3: Update synthetic data and learnable learning rate\n",
    "    total_loss = sum(losses)\n",
    "    total_loss.backward()  # Compute gradients based on real data losses\n",
    "\n",
    "    # Update synthetic data and learning rate\n",
    "    # print(lr.grad) # shouldn't be none\n",
    "    # print(ts_syn.grad) # shouldn't be none\n",
    "    optimizer_ts.step()\n",
    "    optimizer_lr.step()\n",
    "\n",
    "    # Logging the progress\n",
    "    pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\",\n",
    "                      \"learnable lr\": f\"{lr.item()}\",\n",
    "                      })\n",
    "    optim_losses.append(total_loss.item())\n",
    "    syn_lrs.append(lr.item())\n",
    "\n",
    "    if total_loss.item() < min_optim_loss or it >= num_optim_it - 1: # save checkpoint\n",
    "        min_optim_loss = total_loss.item()\n",
    "        print(f\"New best! Saving checkpoint iteration {it}...\")\n",
    "        checkpoint = {\n",
    "            \"it\": it,\n",
    "            \"ts_syn\": ts_syn.detach().clone(),\n",
    "            \"lab_syn\": lab_syn.detach().clone(),\n",
    "            'optim_losses': optim_losses,\n",
    "            'syn_lrs': syn_lrs,\n",
    "            'eval_scores_train': eval_scores_train,\n",
    "            'eval_scores_test': eval_scores_test,\n",
    "        }\n",
    "        # Save checkpoint\n",
    "        torch.save(checkpoint, os.path.join(chckpnt_save_dir, 'distillation_checkpoint.pth'))\n",
    "        print(f\"Checkpoint at iteration {it} saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Distill by matching gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper params\n",
    "\n",
    "train_it = 100\n",
    "lr_data = 0.01\n",
    "lr_net = 0.01\n",
    "episodes_per_cls = 10\n",
    "num_outer_loop = 10\n",
    "num_inner_loop = 50\n",
    "real_batch_size = 256\n",
    "syn_batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize synthetic data\n",
    "\n",
    "episodes_syn = torch.randn(size=(2*episodes_per_cls, input_shape[0], input_shape[1]), dtype=torch.float, requires_grad=True) # device is ignored by far\n",
    "labels_syn = torch.tensor(np.array([np.ones(episodes_per_cls)*i for i in (0, 1)]), dtype=torch.long, requires_grad=False).view(-1) # 1-D, length = episodes_per_cls * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training optimizers and criterion\n",
    "optimizer_ts = torch.optim.SGD([episodes_syn,], lr=lr_data, momentum=0.5) # optimizer for synthetic data\n",
    "optimizer_ts.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with matching loss\n",
    "# TODO: device\n",
    "\n",
    "pkl_save_dir = \"./saved_data\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "pbar = tqdm(range(train_it), desc=\"Training iteration\")\n",
    "for it in pbar:\n",
    "\n",
    "    # Evaluate synthetic data\n",
    "    # TODO\n",
    "\n",
    "    # Train synthetic data\n",
    "    torch.random.manual_seed(int(time.time() * 1000) % 100000) # random init network\n",
    "    net = network.IHMPreliminary1DCNN(input_shape=input_shape)\n",
    "    net.train()\n",
    "    net_params = list(net.parameters())\n",
    "\n",
    "    optimizer_net = torch.optim.SGD(net.parameters(), lr=lr_net)\n",
    "    optimizer_net.zero_grad()\n",
    "    loss_avg = 0\n",
    "\n",
    "    for ol in range(num_outer_loop):\n",
    "        # update synthetic data\n",
    "        loss = torch.tensor(0.0)\n",
    "        for cls in (0, 1):\n",
    "            ts_real, lab_real = train_set.random_sample_from_class(n_samples=real_batch_size, cls=cls)\n",
    "            ts_syn = episodes_syn[cls*episodes_per_cls: (cls+1)*episodes_per_cls]\n",
    "            lab_syn = labels_syn[cls*episodes_per_cls: (cls+1)*episodes_per_cls]\n",
    "\n",
    "            out_real = net(ts_real)\n",
    "            loss_real = criterion(out_real, lab_real)\n",
    "            grad_real = torch.autograd.grad(loss_real, net_params)\n",
    "            grad_real = [_.detach().clone() for _ in grad_real]\n",
    "\n",
    "            out_syn = net(ts_syn)\n",
    "            loss_syn = criterion(out_syn, lab_syn)\n",
    "            grad_syn = torch.autograd.grad(loss_syn, net_params, create_graph=True) # create_graph: will be used to compute higher-order derivatives\n",
    "\n",
    "            # compute gradient matching loss, here using MSE, instead of the one proposed in DCwMG because it's too complicated\n",
    "            dis = torch.tensor(0.0)\n",
    "            grad_real_vec = []\n",
    "            grad_syn_vec = []\n",
    "            for ig in range(len(grad_real)):\n",
    "                grad_real_vec.append(grad_real[ig].reshape((-1)))\n",
    "                grad_syn_vec.append(grad_syn[ig].reshape((-1)))\n",
    "            grad_real_vec = torch.cat(grad_real_vec, dim=0)\n",
    "            grad_syn_vec = torch.cat(grad_syn_vec, dim=0)\n",
    "            dis = torch.sum((grad_syn_vec - grad_real_vec)**2)\n",
    "\n",
    "            loss += dis\n",
    "        \n",
    "        optimizer_ts.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ts.step()\n",
    "        loss_avg += loss.item()\n",
    "\n",
    "        if ol == num_outer_loop - 1:\n",
    "            break\n",
    "\n",
    "        # update network\n",
    "        episodes_syn_train, labels_syn_train = copy.deepcopy(episodes_syn.detach()), copy.deepcopy(labels_syn.detach())  # avoid any unaware modification\n",
    "        syn_dataset = dataset.TensorDataset(episodes_syn_train, labels_syn_train)\n",
    "        train_loader = DataLoader(syn_dataset, syn_batch_size, shuffle=True)\n",
    "        for il in range(num_inner_loop):\n",
    "            train.epoch(\"train\", train_loader, net, optimizer_net, criterion)\n",
    "\n",
    "    loss_avg /= (2 * num_outer_loop)\n",
    "    pbar.set_description(f\"Training iteration, average loss = {loss_avg}\")\n",
    "\n",
    "torch.save({\"data\": (copy.deepcopy(episodes_syn.detach()), copy.deepcopy(labels_syn.detach()))},\n",
    "           os.path.join(pkl_save_dir, \"distilled_dataset.pt\")\n",
    "           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Evaluate distilled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simultaneously train 2 models on distilled dataset and original dataset, compare performance\n",
    "episodes_per_cls = 10\n",
    "\n",
    "real_batch_size = 256\n",
    "syn_batch_size = 2*episodes_per_cls\n",
    "\n",
    "pt_save_dir = \"./saved_data\"\n",
    "syn_pt = \"distilled_dataset.pt\"\n",
    "syn_pt_path = glob(os.path.join(pt_save_dir, syn_pt))[0]\n",
    "syn_data = torch.load(syn_pt_path, map_location=torch.device(device))\n",
    "syn_ts, syn_lab = syn_data[\"data\"]\n",
    "syn_ts = syn_ts.to(device)\n",
    "syn_lab = syn_lab.to(device)\n",
    "syn_set = dataset.TensorDataset(syn_ts, syn_lab)\n",
    "syn_loader = DataLoader(syn_set, 2*episodes_per_cls)\n",
    "\n",
    "num_workers = 8\n",
    "real_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=True,\n",
    "    mask=True,\n",
    "    )\n",
    "real_loader = DataLoader(real_set, real_batch_size, num_workers=num_workers)\n",
    "\n",
    "model_syn = network.IHMPreliminary1DCNN().to(device)\n",
    "model_real = network.IHMPreliminary1DCNN().to(device)\n",
    "\n",
    "train_epoch = 100\n",
    "lr = 0.01\n",
    "\n",
    "optimizer_syn = torch.optim.SGD(model_syn.parameters(), lr)\n",
    "optimizer_syn.zero_grad()\n",
    "optimizer_real = torch.optim.SGD(model_real.parameters(), lr)\n",
    "optimizer_real.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_syn.train()\n",
    "model_real.train()\n",
    "\n",
    "pbar = tqdm(range(train_epoch), desc=\"Evaluating synthetic dataset\")\n",
    "for e in pbar:\n",
    "    train.epoch(\"train\", syn_loader, model_syn, criterion, optimizer_syn, device)\n",
    "    syn_loss, syn_acc = train.epoch(\"test\", real_loader, model_syn, criterion, optimizer_syn, device)\n",
    "    real_loss, real_acc = train.epoch(\"train\", real_loader, model_real, criterion, optimizer_real, device)\n",
    "    \n",
    "    pbar.set_description(f\"Evaluating epoch {e}\\nsyn loss = {syn_loss}, syn acc = {syn_acc}\\nreal loss = {real_loss}, real acc = {real_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
