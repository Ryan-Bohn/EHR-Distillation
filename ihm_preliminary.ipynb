{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary experiment - distilled dataset for in-hospital mortality prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies.\n",
    "\n",
    "For cuda, define `device` that'll be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom libs. **Always re-run the following code block after modifying `utils`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess, dataset, network, train, report\n",
    "importlib.reload(preprocess)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(network)\n",
    "importlib.reload(train)\n",
    "importlib.reload(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute statistics that'll be used for imputation and dataloading.\n",
    "\n",
    "Statistics can be load from saved pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_saved = True\n",
    "stat_pkl_dir = \"./saved_data/stats/\"\n",
    "if not os.path.exists(stat_pkl_dir):\n",
    "    os.makedirs(stat_pkl_dir)\n",
    "\n",
    "categorical_numcls = {  # how many classes are there for categorical classes\n",
    "    \"capillary_refill_rate\": 2,\n",
    "    \"glascow_coma_scale_eye_opening\": 4,\n",
    "    \"glascow_coma_scale_motor_response\": 6,\n",
    "    \"glascow_coma_scale_total\": 13,\n",
    "    \"glascow_coma_scale_verbal_response\": 5,\n",
    "}\n",
    "\n",
    "pkl_path = os.path.join(stat_pkl_dir, \"ihm_preliminary.pkl\")\n",
    "if os.path.exists(pkl_path) and load_from_saved:\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        continuous_avgs_train, continuous_stds_train, categorical_modes_train = pickle.load(f)\n",
    "else:\n",
    "    continuous_avgs_train, continuous_stds_train, categorical_modes_train =  preprocess.compute_feature_statistics(\n",
    "        ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/train/\",\n",
    "        feature_dict=preprocess.mimic3_benchmark_variable_dict\n",
    "        )\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump((continuous_avgs_train, continuous_stds_train, categorical_modes_train), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean all data by resampling, imputating and masking.\n",
    "\n",
    "**Running this block once is enough.**\n",
    "(Cleaned data will be saved at ./data/mimic3/ihm_preliminary/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.preprocess_ihm_timeseries_files(\n",
    "    ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/train/\",\n",
    "    output_dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    feature_dict=preprocess.mimic3_benchmark_variable_dict,\n",
    "    normal_value_dict=continuous_avgs_train|categorical_modes_train\n",
    "    )\n",
    "preprocess.preprocess_ihm_timeseries_files(\n",
    "    ts_dir=\"./data/mimic3/benchmark/in-hospital-mortality/test/\",\n",
    "    output_dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    feature_dict=preprocess.mimic3_benchmark_variable_dict,\n",
    "    normal_value_dict=continuous_avgs_train|categorical_modes_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Evaluate model capacity on objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and evaluation set for IHM objective\n",
    "\n",
    "You may have to re-run this block after modifying dataloader-related codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pay attention to balance and mask settings\n",
    "\n",
    "train_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "print(f\"First item in the dataset: \\n{train_set[0]}\")\n",
    "print(f\"Feature tensor shape: {train_set[0][0].shape}\")\n",
    "\n",
    "test_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    dstype=\"test\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=False,\n",
    "    mask=False,\n",
    "    )\n",
    "print(f\"First item in the dataset: \\n{test_set[0]}\")\n",
    "print(f\"Feature tensor shape: {test_set[0][0].shape}\")\n",
    "\n",
    "input_shape = train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the label distribution in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0_cnt = 0\n",
    "label_1_cnt = 1\n",
    "for _, label in train_set:\n",
    "    if label > 0.5:\n",
    "        label_1_cnt += 1\n",
    "    else:\n",
    "        label_0_cnt += 1\n",
    "print(f\"Label 0 ratio: {label_0_cnt / (label_0_cnt + label_1_cnt)}\")\n",
    "print(f\"Label 1 ratio: {label_1_cnt / (label_0_cnt + label_1_cnt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper params\n",
    "ihm_epoch = 100\n",
    "ihm_batch_size = 256\n",
    "ihm_lr = 0.001\n",
    "ihm_wd = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an 1D CNN and save the best performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1D CNN\n",
    "num_workers = 8\n",
    "pkl_save_dir = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "\n",
    "train_loader = DataLoader(train_set, ihm_batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=num_workers)\n",
    "\n",
    "model = network.IHMPreliminary1DCNN(input_shape=input_shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(ihm_epoch), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, criterion, device=device)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_1dcnn_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(pkl_save_dir, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(pkl_save_dir) if f.startswith(f'ihm_1dcnn_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(pkl_save_dir, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_description(f\"Training on original task, epoch {e}\\ntrain loss = {train_loss}, train acc = {train_acc}\\ntest loss = {test_loss}, test acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an MLP and save the best performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP\n",
    "num_workers = 8\n",
    "pkl_save_dir = \"./saved_data/ihm_model/\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "\n",
    "train_loader = DataLoader(train_set, ihm_batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=num_workers)\n",
    "\n",
    "model = network.IHMPreliminaryMLP(input_shape=input_shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ihm_lr, weight_decay=ihm_wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pbar = tqdm(range(ihm_epoch), desc=\"Training on original task\")\n",
    "min_loss = float(\"inf\")\n",
    "for e in pbar:\n",
    "    train_loss, train_acc = train.epoch(\"train\", train_loader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = train.epoch(\"test\", test_loader, model, criterion, device=device)\n",
    "    if train_loss < min_loss:\n",
    "        filename = f'ihm_mlp_e{e}_trl{train_loss:.4f}_tel{test_loss:.4f}.pt'\n",
    "        file_path = os.path.join(pkl_save_dir, filename)\n",
    "\n",
    "        # Remove the previous checkpoint if it exists\n",
    "        existing_pts = [f for f in os.listdir(pkl_save_dir) if f.startswith(f'ihm_mlp_e{e}_') and f.endswith('.pt')]\n",
    "        for f in existing_pts:\n",
    "            os.remove(os.path.join(pkl_save_dir, f))\n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, file_path)\n",
    "    \n",
    "    pbar.set_description(f\"Training on original task, epoch {e}\\ntrain loss = {train_loss}, train acc = {train_acc}\\ntest loss = {test_loss}, test acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a saved model and evaluate on evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate saved model\n",
    "pt_save_dir = \"./saved_data/ihm_model/\"\n",
    "model_name = \"ihm_1dcnn_*_balanced.pt\"\n",
    "eval_model = network.IHMPreliminary1DCNN(input_shape=(48, 42))\n",
    "\n",
    "model_pt = glob(os.path.join(pt_save_dir, model_name))[0]\n",
    "model_data = torch.load(model_pt, map_location=torch.device(device))\n",
    "\n",
    "eval_model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "eval_model.to(device)\n",
    "eval_model.eval()\n",
    "\n",
    "num_workers = 8\n",
    "balanced_test_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/test/\",\n",
    "    dstype=\"test\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=True,\n",
    "    mask=False,\n",
    "    )\n",
    "test_loader = DataLoader(test_set, ihm_batch_size, num_workers=num_workers) # pay attention to the test set used here\n",
    "\n",
    "report.run_classificatoin_report(eval_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 EHR Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Vanilla DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Dataset Distillation intends to get dataset that is able to train a model within only 1 epoch\n",
    "\n",
    "# define hyper params\n",
    "num_optim_it = 100\n",
    "init_lr = 0.001\n",
    "step_size = 0.001\n",
    "init_weights_distr = \"kaiming\"\n",
    "batch_size = 256\n",
    "num_sampled_models = 16\n",
    "ts_per_cls = 100\n",
    "\n",
    "# initialize random synth dataset\n",
    "ts_syn = torch.randn(size=(2*ts_per_cls, input_shape[0], input_shape[1]), dtype=torch.float, requires_grad=True).to(device) # device is ignored by far\n",
    "lab_syn = torch.tensor(np.array([np.ones(ts_per_cls)*i for i in (0, 1)]), dtype=torch.long, requires_grad=False).view(-1).to(device) # 1-D, length = episodes_per_cls * 2\n",
    "\n",
    "# initialize learning rate\n",
    "lr = torch.tensor([init_lr], dtype=torch.float, requires_grad=True).to(device) # make it learnable\n",
    "\n",
    "optimizer_ts = torch.optim.Adam([ts_syn], lr=step_size)\n",
    "optimizer_lr = torch.optim.Adam([lr], lr=step_size)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# begin training steps\n",
    "pbar = tqdm(range(num_optim_it), desc=\"Training iteration\")\n",
    "for it in pbar:\n",
    "    # get a minibatch of real training data\n",
    "    # Sample from class 0\n",
    "    ts_class_0, lab_class_0 = train_set.random_sample_from_class(n_samples=batch_size//2, cls=0)\n",
    "    # ts_class_0, lab_class_0 = train_set.first_n_samples_from_class(n_samples=batch_size//2, cls=0)\n",
    "    # Sample from class 1\n",
    "    ts_class_1, lab_class_1 = train_set.random_sample_from_class(n_samples=batch_size//2, cls=1)\n",
    "    # ts_class_1, lab_class_1 = train_set.first_n_samples_from_class(n_samples=batch_size//2, cls=1)\n",
    "    # Concatenate the time series data along the first dimension (batch size)\n",
    "    ts_real = torch.cat((ts_class_0, ts_class_1), dim=0).to(device)\n",
    "    # Concatenate the labels along the 0th dimension\n",
    "    lab_real = torch.cat((lab_class_0, lab_class_1), dim=0).to(device)\n",
    "    # print(ts_real.shape, lab_real.shape) # batch_size * num_time_steps * num_features\n",
    "\n",
    "    # sample a batch of models\n",
    "    sampled_models = []\n",
    "    for j in range(num_sampled_models):\n",
    "        torch.random.manual_seed(int(time.time() * 1000) % 100000) # random seed\n",
    "        # torch.random.manual_seed(42) # fixed seed\n",
    "        model = network.IHMPreliminary1DCNN(input_shape=input_shape, init_distr=init_weights_distr).to(device)\n",
    "        model.train()\n",
    "        sampled_models.append(model)\n",
    "        \n",
    "    optimizer_ts.zero_grad()\n",
    "    optimizer_lr.zero_grad()\n",
    "\n",
    "    losses = []\n",
    "    for model in sampled_models:\n",
    "        # Step 1: Train each sampled model on synthetic dataset\n",
    "        pred_syn = model(ts_syn)\n",
    "        loss_syn = loss_fn(pred_syn, lab_syn)\n",
    "        \n",
    "        for m in model.modules():\n",
    "            param_names = []\n",
    "            new_params = []\n",
    "            for n, p in m.named_parameters(recurse=False): # n is the param's name alone instead of \"module.name\"\n",
    "                gp, = torch.autograd.grad(loss_syn, p, create_graph=True) # enabling higher-order derivatives\n",
    "                new_p = p - lr * gp\n",
    "                param_names.append(n) # save them, to delete leaf params later in another enumeration\n",
    "                new_params.append(new_p) # save them, to reset non-leaf params later in another enumeration\n",
    "            for i, n in enumerate(param_names):\n",
    "                delattr(m, n)\n",
    "                setattr(m, n, new_params[i])\n",
    "\n",
    "        # Step 2: Evaluate the objective function on real training data\n",
    "        pred_real = model(ts_real)\n",
    "        loss_real = loss_fn(pred_real, lab_real)\n",
    "        losses.append(loss_real)\n",
    "\n",
    "        # Clear gradients for the next model\n",
    "        model.zero_grad()\n",
    "\n",
    "    # Check if params are swapped as non-leaves\n",
    "    for model in sampled_models:\n",
    "        for m in model.modules():\n",
    "            for n, p in m.named_parameters(recurse=False): # name is the param's name alone instead of module.name\n",
    "                print(p.grad_fn)\n",
    "    \n",
    "    # Step 3: Update synthetic data and learnable learning rate\n",
    "    total_loss = sum(losses)\n",
    "    total_loss.backward()  # Compute gradients based on real data losses\n",
    "\n",
    "    # Update synthetic data and learning rate\n",
    "    # print(lr.grad) # shouldn't be none\n",
    "    # print(ts_syn.grad) # shouldn't be none\n",
    "    optimizer_ts.step()\n",
    "    optimizer_lr.step()\n",
    "\n",
    "    # Logging the progress\n",
    "    pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\",\n",
    "                      \"learnable lr\": f\"{lr.item()}\",\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Distill by Matching Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper params\n",
    "\n",
    "train_it = 100\n",
    "lr_data = 0.01\n",
    "lr_net = 0.01\n",
    "episodes_per_cls = 10\n",
    "num_outer_loop = 10\n",
    "num_inner_loop = 50\n",
    "real_batch_size = 256\n",
    "syn_batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize synthetic data\n",
    "\n",
    "episodes_syn = torch.randn(size=(2*episodes_per_cls, input_shape[0], input_shape[1]), dtype=torch.float, requires_grad=True) # device is ignored by far\n",
    "labels_syn = torch.tensor(np.array([np.ones(episodes_per_cls)*i for i in (0, 1)]), dtype=torch.long, requires_grad=False).view(-1) # 1-D, length = episodes_per_cls * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training optimizers and criterion\n",
    "optimizer_ts = torch.optim.SGD([episodes_syn,], lr=lr_data, momentum=0.5) # optimizer for synthetic data\n",
    "optimizer_ts.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with matching loss\n",
    "# TODO: device\n",
    "\n",
    "pkl_save_dir = \"./saved_data\"\n",
    "if not os.path.exists(pkl_save_dir):\n",
    "    os.makedirs(pkl_save_dir)\n",
    "pbar = tqdm(range(train_it), desc=\"Training iteration\")\n",
    "for it in pbar:\n",
    "\n",
    "    # Evaluate synthetic data\n",
    "    # TODO\n",
    "\n",
    "    # Train synthetic data\n",
    "    torch.random.manual_seed(int(time.time() * 1000) % 100000) # random init network\n",
    "    net = network.IHMPreliminary1DCNN(input_shape=input_shape)\n",
    "    net.train()\n",
    "    net_params = list(net.parameters())\n",
    "\n",
    "    optimizer_net = torch.optim.SGD(net.parameters(), lr=lr_net)\n",
    "    optimizer_net.zero_grad()\n",
    "    loss_avg = 0\n",
    "\n",
    "    for ol in range(num_outer_loop):\n",
    "        # update synthetic data\n",
    "        loss = torch.tensor(0.0)\n",
    "        for cls in (0, 1):\n",
    "            ts_real, lab_real = train_set.random_sample_from_class(n_samples=real_batch_size, cls=cls)\n",
    "            ts_syn = episodes_syn[cls*episodes_per_cls: (cls+1)*episodes_per_cls]\n",
    "            lab_syn = labels_syn[cls*episodes_per_cls: (cls+1)*episodes_per_cls]\n",
    "\n",
    "            out_real = net(ts_real)\n",
    "            loss_real = criterion(out_real, lab_real)\n",
    "            grad_real = torch.autograd.grad(loss_real, net_params)\n",
    "            grad_real = [_.detach().clone() for _ in grad_real]\n",
    "\n",
    "            out_syn = net(ts_syn)\n",
    "            loss_syn = criterion(out_syn, lab_syn)\n",
    "            grad_syn = torch.autograd.grad(loss_syn, net_params, create_graph=True) # create_graph: will be used to compute higher-order derivatives\n",
    "\n",
    "            # compute gradient matching loss, here using MSE, instead of the one proposed in DCwMG because it's too complicated\n",
    "            dis = torch.tensor(0.0)\n",
    "            grad_real_vec = []\n",
    "            grad_syn_vec = []\n",
    "            for ig in range(len(grad_real)):\n",
    "                grad_real_vec.append(grad_real[ig].reshape((-1)))\n",
    "                grad_syn_vec.append(grad_syn[ig].reshape((-1)))\n",
    "            grad_real_vec = torch.cat(grad_real_vec, dim=0)\n",
    "            grad_syn_vec = torch.cat(grad_syn_vec, dim=0)\n",
    "            dis = torch.sum((grad_syn_vec - grad_real_vec)**2)\n",
    "\n",
    "            loss += dis\n",
    "        \n",
    "        optimizer_ts.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ts.step()\n",
    "        loss_avg += loss.item()\n",
    "\n",
    "        if ol == num_outer_loop - 1:\n",
    "            break\n",
    "\n",
    "        # update network\n",
    "        episodes_syn_train, labels_syn_train = copy.deepcopy(episodes_syn.detach()), copy.deepcopy(labels_syn.detach())  # avoid any unaware modification\n",
    "        syn_dataset = dataset.TensorDataset(episodes_syn_train, labels_syn_train)\n",
    "        train_loader = DataLoader(syn_dataset, syn_batch_size, shuffle=True)\n",
    "        for il in range(num_inner_loop):\n",
    "            train.epoch(\"train\", train_loader, net, optimizer_net, criterion)\n",
    "\n",
    "    loss_avg /= (2 * num_outer_loop)\n",
    "    pbar.set_description(f\"Training iteration, average loss = {loss_avg}\")\n",
    "\n",
    "torch.save({\"data\": (copy.deepcopy(episodes_syn.detach()), copy.deepcopy(labels_syn.detach()))},\n",
    "           os.path.join(pkl_save_dir, \"distilled_dataset.pt\")\n",
    "           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Evaluate distilled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simultaneously train 2 models on distilled dataset and original dataset, compare performance\n",
    "episodes_per_cls = 10\n",
    "\n",
    "real_batch_size = 256\n",
    "syn_batch_size = 2*episodes_per_cls\n",
    "\n",
    "pt_save_dir = \"./saved_data\"\n",
    "syn_pt = \"distilled_dataset.pt\"\n",
    "syn_pt_path = glob(os.path.join(pt_save_dir, syn_pt))[0]\n",
    "syn_data = torch.load(syn_pt_path, map_location=torch.device(device))\n",
    "syn_ts, syn_lab = syn_data[\"data\"]\n",
    "syn_ts = syn_ts.to(device)\n",
    "syn_lab = syn_lab.to(device)\n",
    "syn_set = dataset.TensorDataset(syn_ts, syn_lab)\n",
    "syn_loader = DataLoader(syn_set, 2*episodes_per_cls)\n",
    "\n",
    "num_workers = 8\n",
    "real_set = dataset.IHMPreliminaryDatasetReal(\n",
    "    dir=\"./data/mimic3/ihm_preliminary/train/\",\n",
    "    dstype=\"train\",\n",
    "    avg_dict=continuous_avgs_train,\n",
    "    std_dict=continuous_stds_train,\n",
    "    numcls_dict=categorical_numcls,\n",
    "    balance=True,\n",
    "    mask=True,\n",
    "    )\n",
    "real_loader = DataLoader(real_set, real_batch_size, num_workers=num_workers)\n",
    "\n",
    "model_syn = network.IHMPreliminary1DCNN().to(device)\n",
    "model_real = network.IHMPreliminary1DCNN().to(device)\n",
    "\n",
    "train_epoch = 100\n",
    "lr = 0.01\n",
    "\n",
    "optimizer_syn = torch.optim.SGD(model_syn.parameters(), lr)\n",
    "optimizer_syn.zero_grad()\n",
    "optimizer_real = torch.optim.SGD(model_real.parameters(), lr)\n",
    "optimizer_real.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_syn.train()\n",
    "model_real.train()\n",
    "\n",
    "pbar = tqdm(range(train_epoch), desc=\"Evaluating synthetic dataset\")\n",
    "for e in pbar:\n",
    "    train.epoch(\"train\", syn_loader, model_syn, criterion, optimizer_syn, device)\n",
    "    syn_loss, syn_acc = train.epoch(\"test\", real_loader, model_syn, criterion, optimizer_syn, device)\n",
    "    real_loss, real_acc = train.epoch(\"train\", real_loader, model_real, criterion, optimizer_real, device)\n",
    "    \n",
    "    pbar.set_description(f\"Evaluating epoch {e}\\nsyn loss = {syn_loss}, syn acc = {syn_acc}\\nreal loss = {real_loss}, real acc = {real_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
